{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cardiotoxicity](cardiotoxicityProjectTitle.jpg)\n",
    "#MALDI Parser\n",
    "![maldi plate](maldi_anchor_s.jpg)\n",
    "\n",
    "#Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Functions\n",
    "- Finding missed peak(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding Missed Peak\n",
    "def detect_missed(mzs):\n",
    "    mzs_missing=[]\n",
    "    \n",
    "    for normal_mz in normal_mzs:\n",
    "        for mz in mzs:\n",
    "            if mz<normal_mz+6 and mz>normal_mz-6:   #Searching in this range\n",
    "                break\n",
    "        else:\n",
    "            mzs_missing.append(normal_mz)\n",
    "    return mzs_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding redundant peak(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding Redundant Peak\n",
    "def detect_redundant(mzs):\n",
    "    mzs_adjacents=[]    \n",
    "\n",
    "    for normal_mz in normal_mzs:\n",
    "        d=9.99\n",
    "        m=0\n",
    "        for mz in mzs:\n",
    "            if abs(normal_mz-mz)<d:\n",
    "                d=abs(normal_mz-mz)\n",
    "                m=mz\n",
    "        mzs_adjacents.append(m)\n",
    "   \n",
    "    return mzs_adjacents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Layout Display\n",
    "def layout(names):\n",
    "    layout_names=[]\n",
    "    layout_series=[]\n",
    "    for name in names:\n",
    "        patt4=r'[\\D\\d]+_0_'\n",
    "        sample_name=re.findall(patt4, name)[0].replace('_0_', '').replace('-', '_')\n",
    "        \n",
    "        if sample_name.find('standard')!=-1 or sample_name.find('Standard')!=-1:\n",
    "            sample_name=sample_name.replace('standard', '').replace('Standard', '').replace('_', '')       \n",
    "\n",
    "        patt5=r'_0_[\\D\\d]+_1$'\n",
    "        id_name=re.findall(patt5, name)[0].replace('_0_', '').replace('_1', '')\n",
    "\n",
    "        if len(id_name)==2:\n",
    "            id_name=id_name[0]+'0'+id_name[1]\n",
    "\n",
    "         \n",
    "        layout_names.append((id_name, sample_name))\n",
    "\n",
    "    d=96-len(layout_names)\n",
    "\n",
    "    for i in range(d):\n",
    "        layout_names.append(('Z'+str(i),np.nan))\n",
    "\n",
    "    sample_dict=dict(layout_names)\n",
    "    s=pd.Series(sample_dict).sort_index()\n",
    "    df=pd.DataFrame(np.array(s).reshape(8,12),\n",
    "                           columns=[1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "                           index=['A','B','C','D','E','F','G','H'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change certain value into NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make certain cells blank (NaN)\n",
    "def blank():\n",
    "    df['SAA std'][df['level_1']==1]=np.nan\n",
    "    df['SAA std'][df['level_1']==2]=np.nan\n",
    "    df['SAA std'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['SAA avg'][df['level_1']==1]=np.nan\n",
    "    df['SAA avg'][df['level_1']==2]=np.nan\n",
    "    df['SAA avg'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['hep std'][df['level_1']==1]=np.nan\n",
    "    df['hep std'][df['level_1']==2]=np.nan\n",
    "    df['hep std'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['hep avg'][df['level_1']==1]=np.nan\n",
    "    df['hep avg'][df['level_1']==2]=np.nan\n",
    "    df['hep avg'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['SAA/IS_SAA'][df['level_1']==1]=np.nan\n",
    "    df['SAA/IS_SAA'][df['level_1']==2]=np.nan\n",
    "    df['SAA/IS_SAA'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['hep/hhep'][df['level_1']==1]=np.nan\n",
    "    df['hep/hhep'][df['level_1']==2]=np.nan\n",
    "    df['hep/hhep'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['SAA CV'][df['level_1']==1]=np.nan\n",
    "    df['SAA CV'][df['level_1']==2]=np.nan\n",
    "    df['SAA CV'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['hep CV'][df['level_1']==1]=np.nan\n",
    "    df['hep CV'][df['level_1']==2]=np.nan\n",
    "    df['hep CV'][df['level_1']==3]=np.nan\n",
    "\n",
    "    df['name'][df['level_1']==1]=np.nan\n",
    "    df['name'][df['level_1']==2]=np.nan\n",
    "    df['name'][df['level_1']==3]=np.nan\n",
    "        \n",
    "    df['hep avg'][df['Tail']%2==0]=np.nan\n",
    "    df['SAA avg'][df['Tail']%2==0]=np.nan\n",
    "    df['hep std'][df['Tail']%2==0]=np.nan\n",
    "    df['SAA std'][df['Tail']%2==0]=np.nan\n",
    "    df['hep CV'][df['Tail']%2==0]=np.nan\n",
    "    df['SAA CV'][df['Tail']%2==0]=np.nan\n",
    "\n",
    "    df['tag'][df['level_1']==1]=np.nan\n",
    "    df['tag'][df['level_1']==2]=np.nan\n",
    "    df['tag'][df['level_1']==3]=np.nan\n",
    "    df['tag'][df['Tail']%2==0]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Main Program Flow\n",
    "We chose four peaks for detection: **1526, 1536** (Hepcidin), **2791, 2819** (SAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#////////////////MAIN///////////////////////////////////////////////\n",
    "#Ignoring Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Default Peaks\n",
    "normal_mzs=[1526, 1536, 2791, 2819]\n",
    "normal_rows=len(normal_mzs)\n",
    "\n",
    "#Import File\n",
    "filename='Raw MALDI.xlsx'\n",
    "book=xlrd.open_workbook(filename)\n",
    "print('Source file: '+sys.path[0]+filename+' loaded!')\n",
    "\n",
    "#Extraction\n",
    "nsheets=book.nsheets\n",
    "\n",
    "sheet_names=book.sheet_names()\n",
    "sheets={}\n",
    "\n",
    "for sheet_name in sheet_names:    \n",
    "    nrows=book.sheet_by_name(sheet_name).nrows\n",
    "    current_header=book.sheet_by_name(sheet_name).row_values(2) \n",
    "    current_data=[book.sheet_by_name(sheet_name).row_values(i) for i in range(3, nrows)]\n",
    "    sheets[sheet_name]=pd.DataFrame(current_data, columns=current_header)   #DataFrame Construction\n",
    "#Feedback\n",
    "print('Data Extracted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dealing with the Missing Peak(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#///////////////////Main Loop///////////\n",
    "peak_missing_amount=0\n",
    "peak_redundant_amount=0\n",
    "peak_redundant_item=[]\n",
    "peak_missing_item=[]\n",
    "peak_repeated_amount=0\n",
    "peak_repeated_item=[]\n",
    "\n",
    "for sheet_name in sheet_names:\n",
    "    df=sheets[sheet_name]\n",
    "\n",
    "    actual_rows=len(df.index)\n",
    "    mzs=list(df['m/z'])\n",
    "\n",
    "    #////////////////////Unique!!!!///////////////////////////////////////////////\n",
    "    for mz in mzs:\n",
    "        if len(df[df['m/z']==mz].index)>1:\n",
    "            del_index=list(df[df['m/z']==mz].index)\n",
    "            del_index.pop(0)\n",
    "            df=df.drop(del_index)\n",
    "\n",
    "            mzs=list(df['m/z'])\n",
    "\n",
    "            peak_repeated_amount+=1\n",
    "            peak_repeated_item.append(sheet_name.replace('_0_', ' @ ').replace('_1', ''))\n",
    "    #////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    df=df.sort_index(by='m/z')\n",
    "    df=df.reset_index().drop('index', axis=1)\n",
    "\n",
    "    actual_rows=len(df.index)\n",
    "\n",
    "    #///////Larger than normal: Redundant///////////\n",
    "    if actual_rows>normal_rows:\n",
    "        mzs=list(df['m/z'])\n",
    "        mzs_adjacents=detect_redundant(mzs)    #Call Function          \n",
    "        df=pd.concat([df.ix[df[df['m/z']==mzs_adjacent].index] for mzs_adjacent in mzs_adjacents])        \n",
    "\n",
    "        #////Memorize the Redundant One//////////////////    \n",
    "        peak_redundant_amount+=1\n",
    "        peak_redundant_item.append(sheet_name.replace('_0_', ' @ ').replace('_1', ''))\n",
    "\n",
    "    #/////Sort & Reindex////////////\n",
    "    df=df.sort_index(by='m/z')\n",
    "    df=df.reset_index().drop('index', axis=1)\n",
    "\n",
    "    actual_rows=len(df.index)\n",
    "    \n",
    "    ##///////Less than normal: Missing///////\n",
    "    if actual_rows<normal_rows:\n",
    "        mzs=list(df['m/z'])\n",
    "        mzs_missing=detect_missed(mzs)  #Call Function \n",
    "\n",
    "        i=actual_rows\n",
    "        \n",
    "        for mz_missing in mzs_missing:\n",
    "            df.ix[i]=0\n",
    "            df['m/z'].ix[i]=mz_missing\n",
    "            i+=1\n",
    "\n",
    "        #////Memorize the Missing One///////\n",
    "        peak_missing_amount+=1\n",
    "        peak_missing_item.append(sheet_name.replace('_0_', ' @ ').replace('_1', ''))\n",
    "\n",
    "    df=df.sort_index(by='m/z')\n",
    "    df=df.reset_index().drop('index', axis=1)\n",
    "\n",
    "    actual_rows=len(df.index)    \n",
    "\n",
    "    #///////Again! Larger than normal: Redundant////////\n",
    "    if actual_rows>normal_rows:\n",
    "        mzs=list(df['m/z'])\n",
    "        mzs_adjacents=detect_redundant(mzs)     #Call Function          \n",
    "        df=pd.concat([df.ix[df[df['m/z']==mzs_adjacent].index] for mzs_adjacent in mzs_adjacents])       \n",
    "\n",
    "        #/Memorize the Redundant One\n",
    "        peak_redundant_amount+=1\n",
    "        peak_redundant_item.append(sheet_name.replace('_0_', ' @ ').replace('_1', ''))\n",
    "\n",
    "    #Sort & Reindex/////////////\n",
    "    df=df.sort_index(by='m/z')\n",
    "    df=df.reset_index().drop('index', axis=1)\n",
    "\n",
    "    #Descriptive Calculation\n",
    "    hep_ratio=df['Area'].ix[2]/df['Area'].ix[3]\n",
    "    saa_ratio=df['Area'].ix[0]/df['Area'].ix[1]\n",
    "\n",
    "    #Regular Expressions////Matching Sample Names\n",
    "    patt=r'[\\D\\d]+_0_'\n",
    "    name=re.findall(patt, sheet_name)[0].replace('_0_', '').replace('-', '_')\n",
    "\n",
    "    #Regular Expressions////Matching Vial NOs\n",
    "    patt1=r'_0_[\\D\\d]+_1$'\n",
    "    id=re.findall(patt1, sheet_name)[0].replace('_0_', '').replace('_1', '')\n",
    "\n",
    "    #Regular Expressions////Matching Tails of the Vials\n",
    "    patt2=r'\\d+$'\n",
    "    tail=int(re.findall(patt2, id)[0])\n",
    "\n",
    "    if len(id)==2:\n",
    "        id=id[0]+'0'+id[1]\n",
    "\n",
    "    df['Tail']=tail\n",
    "    df['id']=id\n",
    "    df['name']=name\n",
    "    df['hep/hhep']=hep_ratio\n",
    "    df['SAA/IS_SAA']=saa_ratio\n",
    "    df['tag']=name\n",
    "\n",
    "    sheets[sheet_name]=df\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Combination & Unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combining DataFrame\n",
    "#Feedback: Combining\n",
    "print('Combining...')\n",
    "df=pd.concat([sheets[sheet_name] for sheet_name in sheet_names], keys=sheet_names)\n",
    "\n",
    "#Feedback: Start Parsing\n",
    "print('Parsing...')\n",
    "\n",
    "#去重\n",
    "names=list(df['name'].unique())\n",
    "\n",
    "#Description Statistics Initialization\n",
    "df['hep avg']=0\n",
    "df['hep std']=0\n",
    "df['hep CV']=0\n",
    "\n",
    "df['SAA avg']=0\n",
    "df['SAA std']=0\n",
    "df['SAA CV']=0\n",
    "\n",
    "#///Change the sheetnames from indeces into column names\n",
    "df=df.reset_index()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Descriptive Statistics for the same names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#////////Loop of Processing Data in the Same Names///////////////\n",
    "for name in names:\n",
    "    avg=df['hep/hhep'][df['name']==name].mean()\n",
    "    df['hep avg'][df['name']==name]=avg\n",
    "\n",
    "    std=df['hep/hhep'][df['name']==name].std()\n",
    "    df['hep std'][df['name']==name]=std\n",
    "\n",
    "    cv=std*100/avg\n",
    "    df['hep CV'][df['name']==name]=cv\n",
    "\n",
    "    #////////////////////////////////////////////\n",
    "    \n",
    "    avg=df['SAA/IS_SAA'][df['name']==name].mean()\n",
    "    df['SAA avg'][df['name']==name]=avg\n",
    "\n",
    "    std=df['SAA/IS_SAA'][df['name']==name].std()\n",
    "    df['SAA std'][df['name']==name]=std\n",
    "\n",
    "    cv=std*100/avg\n",
    "    df['SAA CV'][df['name']==name]=cv\n",
    "\n",
    "    #/////////////////////////////////////////////\n",
    "\n",
    "    if name.find('standard') != -1 or name.find('Standard') != -1:\n",
    "        tag=name.replace('standard', '').replace('Standard', '').replace('_', '').replace(' ', '')\n",
    "        df['tag'][df['name']==name]=tag        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Export the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calling Function\n",
    "print('Trimming...')\n",
    "blank()\n",
    "\n",
    "#Sorting\n",
    "df=df.sort_index(by=['id', 'level_1'])\n",
    "df=df.set_index('level_0')\n",
    "\n",
    "#Output\n",
    "print('Exporting...')\n",
    "df.to_csv('.\\\\combined.csv')\n",
    "\n",
    "dfl=layout(sheet_names)\n",
    "dfl.to_csv('.\\\\layout.csv')\n",
    "print('Files Exported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final Feedback\n",
    "print('\\nDone!\\n')\n",
    "\n",
    "print('Total Sample Amount: ', nsheets)\n",
    "\n",
    "print('\\tPeak Missing: ', peak_missing_amount)\n",
    "print('\\t\\tMissing Item(s): ', peak_missing_item, '\\n')\n",
    "\n",
    "print('\\tPeak Redundant: ', peak_redundant_amount)\n",
    "print('\\t\\tRedundant Item(s): ', peak_redundant_item, '\\n')\n",
    "\n",
    "print('\\tPeak Repeated: ', peak_repeated_amount)\n",
    "print('\\t\\tRepeated Item(s): ', peak_repeated_item, '\\n')\n",
    "\n",
    "print('Combined Document Exported: ', sys.path[0]+'\\\\combined.csv')\n",
    "print('Layout Document Exported: ', sys.path[0]+'\\\\layout.csv\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Drawing Standard Curve\n",
    "- Extraction of Standard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standard_list=['250', '125', '62.5', '31.25', '15.6', '7.8', '3.9', '1.9', '0']\n",
    "standard_value={}\n",
    "\n",
    "for current in standard_list:\n",
    "    standard_value[float(current)]=float(df[df['tag']==current]['hep avg'])\n",
    "concentration=list(standard_value.keys())\n",
    "intensity=list(standard_value.values())\n",
    "\n",
    "regression_result=stats.linregress(concentration, intensity)\n",
    "corr_result=stats.pearsonr(concentration, intensity)\n",
    "\n",
    "x=range(-50, 300)\n",
    "y=regression_result[0]*x+regression_result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting Scatters and Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(concentration, intensity)\n",
    "plt.plot(x, y, color='r')\n",
    "plt.savefig('StandardCurve.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showing the Correlation Coefficient and P Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('r=', corr_result[0])\n",
    "print('P=', corr_result[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
